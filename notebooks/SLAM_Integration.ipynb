{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "r8tz3oBrHDJ_",
      "metadata": {
        "id": "r8tz3oBrHDJ_"
      },
      "source": [
        "<h1><b>SLAM INTEGRATION</h1></b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "l_d9S3nVXK1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_d9S3nVXK1e",
        "outputId": "bf09dffe-0e66-4cb8-a9e1-72b97c056a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /Users/quocanh/anaconda3/lib/python3.11/site-packages (8.3.167)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.2.6)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (3.8.4)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.32.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.3.0)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (5.9.0)\n",
            "Requirement already satisfied: py-cpuinfo in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.3.3)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: sympy in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2024.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/quocanh/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a57e2d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict, defaultdict\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04d15a0",
      "metadata": {},
      "source": [
        "<h2>Path Declaration</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f115017",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "model_weights = '../results/yolov8s/kitti/best.pt'\n",
        "calib_file = Path('../data/data_odometry_gray/sequences/08/calib.txt')\n",
        "seq_dir = Path('../data/data_odometry_gray/sequences/08/image_0')\n",
        "pose_dir = Path('../data/data_odometry_poses/poses/08.txt')\n",
        "bbox_output_dir = Path('../data/bbox_outputs')\n",
        "cuboid_output_dir = Path('../data/cuboid_outputs')\n",
        "# bbox_output_dir = Path('../data/bbox_outputs_bytetrack')\n",
        "# cuboid_output_dir = Path('../data/cuboid_outputs_bytetrack')\n",
        "# calib_file = Path('../data/adelaide_sequence/calib.txt')\n",
        "# seq_dir = Path('../data/adelaide_sequence/image_0')\n",
        "# bbox_output_dir = Path('../data/bbox_outputs_adelaide')\n",
        "# cuboid_output_dir = Path('../data/cuboid_outputs_adelaide')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31312d10",
      "metadata": {},
      "source": [
        "<h2>Helper Function</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a2da1520",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded camera intrinsics fx=707.09, fy=707.09, cx=601.89, cy=183.11\n"
          ]
        }
      ],
      "source": [
        "# 1. Load camera intrinsic parameters from calib.txt (using P2 matrix for left camera)\n",
        "with open(calib_file, 'r') as f:\n",
        "    calib_lines = f.readlines()\n",
        "# Find the line starting with 'P2:'\n",
        "P2_line = next(line for line in calib_lines if line.startswith('P2:'))\n",
        "P2_vals = P2_line.strip().split()[1:]  # skip 'P2:'\n",
        "P2 = np.array(list(map(float, P2_vals))).reshape(3, 4)  # 3x4 projection matrix\n",
        "# Intrinsic matrix K is the left 3x3 part of P2\n",
        "K   = P2[:, :3]\n",
        "fx  = K[0, 0]; fy = K[1, 1]\n",
        "cx  = K[0, 2]; cy = K[1, 2]\n",
        "print(f'Loaded camera intrinsics fx={fx:.2f}, fy={fy:.2f}, cx={cx:.2f}, cy={cy:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3c19e9ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: project 3D cuboid corners given center (X,Y,Z) and yaw in camera frame\n",
        "def project_cuboid(X, Y, Z, W, H, L, yaw):\n",
        "    \"\"\"\n",
        "    Compute the 2D bounding box (min/max u,v) of a cuboid with given center, size, and yaw (rotation about vertical axis),\n",
        "    projected into the camera image.\n",
        "    \"\"\"\n",
        "    # Rotation matrix around camera Y-axis (assumed vertical in camera coords) by yaw\n",
        "    c, s = np.cos(yaw), np.sin(yaw)\n",
        "    R_yaw = np.array([\n",
        "        [ c, 0, s],\n",
        "        [ 0, 1, 0],\n",
        "        [-s, 0, c]\n",
        "    ])  # rotates object local coords into camera coords\n",
        "    # Eight corners of cuboid in object local coordinates (centered at origin)\n",
        "    corners_local = np.array([\n",
        "        [dx, dy, dz]\n",
        "        for dx in (-W/2, W/2)\n",
        "        for dy in (-H/2, H/2)\n",
        "        for dz in (-L/2, L/2)\n",
        "    ])\n",
        "    # Transform corners to camera coordinates\n",
        "    corners_cam = corners_local.dot(R_yaw.T) + np.array([X, Y, Z])\n",
        "    # Project to image pixels\n",
        "    us = fx * (corners_cam[:, 0] / corners_cam[:, 2]) + cx\n",
        "    vs = fy * (corners_cam[:, 1] / corners_cam[:, 2]) + cy\n",
        "    u_min, u_max = us.min(), us.max()\n",
        "    v_min, v_max = vs.min(), vs.max()\n",
        "    return u_min, u_max, v_min, v_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "62a4d333",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define average object dimensions per class (width, height, length in meters)\n",
        "\n",
        "# Using KITTI dataset stats:contentReference[oaicite:8]{index=8} for Car/Pedestrian/Cyclist as examples\n",
        "class_dims = {\n",
        "    0: {'name': 'Car',          'dims': (1.6, 1.5, 3.9)},   # width, height, length\n",
        "    1: {'name': 'Pedestrian',   'dims': (0.6, 1.7, 0.6)},   # human body\n",
        "    2: {'name': 'Cyclist',      'dims': (0.6, 1.7, 1.5)},   # person + bicycle\n",
        "    3: {'name': 'Lane',         'dims': (3.5, 0.1, 50.0)},  # typical lane width ~3.5m, very flat & long\n",
        "    4: {'name': 'Traffic Sign', 'dims': (0.8, 2.0, 0.2)},   # pole-mounted sign (width ~0.8m, height ~2m)\n",
        "    5: {'name': 'Traffic Light','dims': (0.4, 1.0, 0.4)},   # pole-mounted light cluster\n",
        "    6: {'name': 'Drivable Area','dims': (6.0, 0.1, 50.0)},  # approximate: wide, flat road patch\n",
        "    7: {'name': 'Truck',        'dims': (2.5, 3.5, 12.0)},  # semi-truck\n",
        "    8: {'name': 'Bus',          'dims': (2.5, 3.0, 12.0)},  # city bus\n",
        "    9: {'name': 'Bike',         'dims': (0.6, 1.2, 1.8)},   # standalone bicycle\n",
        "   10: {'name': 'Motor',        'dims': (0.8, 1.4, 2.2)},   # motorcycle + rider\n",
        "   11: {'name': 'Train',        'dims': (3.2, 4.5, 30.0)}   # single carriage segment\n",
        "}\n",
        "\n",
        "def estimate_cuboid(bbox, class_id):\n",
        "    \"\"\"\n",
        "    Estimate 3D cuboid from 2D bounding box.\n",
        "    \n",
        "    Args:\n",
        "        bbox: [x_min, y_min, x_max, y_max] or list\n",
        "        class_id: Object class ID\n",
        "    \n",
        "    Returns:\n",
        "        dict: Cuboid parameters (center, dimensions, orientation, corners)\n",
        "    \"\"\"\n",
        "    # Convert bbox to array if needed\n",
        "    if isinstance(bbox, list):\n",
        "        bbox = np.array(bbox)\n",
        "    \n",
        "    x1, y1, x2, y2 = bbox\n",
        "    \n",
        "    # Get class dimensions or default\n",
        "    class_info = class_dims.get(int(class_id), {'name': 'Unknown', 'dims': (1.0, 1.0, 1.0)})\n",
        "    W, H, L = class_info['dims']\n",
        "    \n",
        "    # Depth estimation from height\n",
        "    pixel_height = y2 - y1\n",
        "    Z = (fy * H) / (pixel_height + 1e-6)\n",
        "    \n",
        "    # Horizontal position\n",
        "    u_center = 0.5 * (x1 + x2)\n",
        "    X = (u_center - cx) / fx * Z\n",
        "    \n",
        "    # Vertical position\n",
        "    v_bottom = max(y1, y2)\n",
        "    Y = (v_bottom - cy) / fy * Z - 0.5 * H\n",
        "    \n",
        "    # Orientation estimation via search\n",
        "    pixel_width = x2 - x1  \n",
        "    best_yaw = 0.0\n",
        "    min_err = float('inf')\n",
        "    for deg in range(0, 91, 5):\n",
        "        yaw_cand = np.deg2rad(deg)\n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw_cand)\n",
        "        err = abs((u_max - u_min) - pixel_width)\n",
        "        if err < min_err:\n",
        "            min_err = err\n",
        "            best_yaw = yaw_cand\n",
        "    \n",
        "    # Refine yaw\n",
        "    yaw = best_yaw\n",
        "    for _ in range(3):\n",
        "        delta = np.deg2rad(2)\n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw)\n",
        "        err0 = abs((u_max - u_min) - pixel_width)\n",
        "        \n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw + delta)\n",
        "        err_plus = abs((u_max - u_min) - pixel_width)\n",
        "        \n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw - delta)\n",
        "        err_minus = abs((u_max - u_min) - pixel_width)\n",
        "        \n",
        "        if err_plus < err0 or err_minus < err0:\n",
        "            yaw += delta if err_plus < err_minus else -delta\n",
        "        else:\n",
        "            break\n",
        "    \n",
        "    # Center alignment adjustment\n",
        "    u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw)\n",
        "    proj_center = 0.5 * (u_min + u_max)\n",
        "    X += ((u_center - proj_center) / fx) * Z\n",
        "    \n",
        "    # Compute 3D corners\n",
        "    c, s = np.cos(yaw), np.sin(yaw)\n",
        "    R_yaw = np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]])\n",
        "    \n",
        "    corners_local = np.array([\n",
        "        [dx, dy, dz]\n",
        "        for dx in (-W/2, W/2)\n",
        "        for dy in (-H/2, H/2)\n",
        "        for dz in (-L/2, L/2)\n",
        "    ])\n",
        "    \n",
        "    corners_3d = corners_local.dot(R_yaw.T) + np.array([X, Y, Z])\n",
        "    \n",
        "    return {\n",
        "        'center': [float(X), float(Y), float(Z)],\n",
        "        'dimensions': [float(W), float(H), float(L)],\n",
        "        'orientation': float(yaw),\n",
        "        'rotation': {  # Quaternion (for compatibility)\n",
        "            'w': float(np.cos(yaw/2)),\n",
        "            'x': 0.0,\n",
        "            'y': float(np.sin(yaw/2)),\n",
        "            'z': 0.0\n",
        "        },\n",
        "        'corners_3d': corners_3d.tolist(),\n",
        "        'class_id': int(class_id),\n",
        "        'class_name': class_info['name'],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c10b15",
      "metadata": {},
      "source": [
        "<h2>1. ByteTrack Tracker Implementation</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d80528",
      "metadata": {},
      "source": [
        "**Multi-object tracker to maintain object identity across frames** \n",
        "\n",
        "```\n",
        "KITTI Images → YOLOv8 → [NEW: ByteTrack] → 3D Cuboids (cached) → ORB-SLAM3\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "20ae390b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ByteTrack tracker classes loaded!\n"
          ]
        }
      ],
      "source": [
        "class SimpleByteTracker:\n",
        "    \"\"\"ByteTrack implementation for maintaining object IDs across frames.\"\"\"\n",
        "    \n",
        "    def __init__(self, track_thresh=0.5, track_buffer=30, match_thresh=0.8, min_box_area=100):\n",
        "        self.track_thresh = track_thresh\n",
        "        self.track_buffer = track_buffer\n",
        "        self.match_thresh = match_thresh\n",
        "        self.min_box_area = min_box_area\n",
        "        self.tracked_tracks = OrderedDict()\n",
        "        self.lost_tracks = OrderedDict()\n",
        "        self.removed_tracks = OrderedDict()\n",
        "        self.frame_id = 0\n",
        "        self.track_id_count = 0\n",
        "    \n",
        "    def update(self, detections):\n",
        "        \"\"\"Update tracker with new detections.\"\"\"\n",
        "        self.frame_id += 1\n",
        "        if len(detections) == 0:\n",
        "            detections = np.empty((0, 6))\n",
        "        \n",
        "        valid_detections = self._filter_detections(detections)\n",
        "        high_det = valid_detections[valid_detections[:, 4] >= self.track_thresh]\n",
        "        low_det = valid_detections[valid_detections[:, 4] < self.track_thresh]\n",
        "        tracks_output = []\n",
        "        \n",
        "        if len(self.tracked_tracks) > 0:\n",
        "            # Get list of current track IDs\n",
        "            track_ids = list(self.tracked_tracks.keys())\n",
        "            \n",
        "            # Associate high-confidence detections with existing tracks\n",
        "            matched, unmatched_tracks, unmatched_dets = self._associate(self.tracked_tracks, high_det)\n",
        "            \n",
        "            # Update matched tracks\n",
        "            for track_idx, det_idx in matched:\n",
        "                track_id = track_ids[track_idx]\n",
        "                track = self.tracked_tracks[track_id]  # ✅ Use track_id\n",
        "                det = high_det[det_idx]\n",
        "                track.update({\n",
        "                    'bbox': det[:4].tolist(),\n",
        "                    'score': float(det[4]),\n",
        "                    'frame_id': self.frame_id,\n",
        "                    'state': 'tracked'\n",
        "                })\n",
        "                tracks_output.append(track.copy())\n",
        "            \n",
        "            # Move unmatched tracks to lost\n",
        "            for track_idx in unmatched_tracks:\n",
        "                track_id = track_ids[track_idx]\n",
        "                track = self.tracked_tracks[track_id]  # ✅ Use track_id\n",
        "                track['state'] = 'lost'\n",
        "                self.lost_tracks[track['track_id']] = track\n",
        "            \n",
        "            # Delete unmatched tracks from tracked_tracks\n",
        "            for track_idx in unmatched_tracks:\n",
        "                track_id = track_ids[track_idx]\n",
        "                del self.tracked_tracks[track_id]  # ✅ Use track_id\n",
        "            \n",
        "            # Try to recover lost tracks with low-confidence detections\n",
        "            if len(low_det) > 0 and len(self.lost_tracks) > 0:\n",
        "                lost_track_ids = list(self.lost_tracks.keys())\n",
        "                matched_lost, _, _ = self._associate(self.lost_tracks, low_det)\n",
        "                \n",
        "                for track_idx, det_idx in matched_lost:\n",
        "                    track_id = lost_track_ids[track_idx]\n",
        "                    track = self.lost_tracks[track_id]  # ✅ Use track_id\n",
        "                    det = low_det[det_idx]\n",
        "                    track.update({\n",
        "                        'bbox': det[:4].tolist(),\n",
        "                        'score': float(det[4]),\n",
        "                        'frame_id': self.frame_id,\n",
        "                        'state': 're-identified'\n",
        "                    })\n",
        "                    self.tracked_tracks[track_id] = track  # ✅ Use track_id\n",
        "                    tracks_output.append(track.copy())\n",
        "                    del self.lost_tracks[track_id]  # ✅ Use track_id\n",
        "            \n",
        "            # Initialize new tracks for unmatched high-confidence detections\n",
        "            for det_idx in unmatched_dets:\n",
        "                new_track = self._init_track(high_det[det_idx])\n",
        "                self.tracked_tracks[new_track['track_id']] = new_track\n",
        "                tracks_output.append(new_track.copy())\n",
        "        else:\n",
        "            # First frame - initialize all high-confidence detections as new tracks\n",
        "            for det in high_det:\n",
        "                new_track = self._init_track(det)\n",
        "                self.tracked_tracks[new_track['track_id']] = new_track\n",
        "                tracks_output.append(new_track.copy())\n",
        "        \n",
        "        # Remove old lost tracks that exceeded buffer time\n",
        "        lost_to_remove = []\n",
        "        for track_id, track in self.lost_tracks.items():\n",
        "            if self.frame_id - track['frame_id'] > self.track_buffer:\n",
        "                lost_to_remove.append(track_id)\n",
        "                self.removed_tracks[track_id] = track\n",
        "        \n",
        "        for track_id in lost_to_remove:\n",
        "            del self.lost_tracks[track_id]\n",
        "        \n",
        "        return tracks_output\n",
        "    \n",
        "    def _filter_detections(self, detections):\n",
        "        \"\"\"Filter out detections with too small area.\"\"\"\n",
        "        if len(detections) == 0:\n",
        "            return detections\n",
        "        areas = (detections[:, 2] - detections[:, 0]) * (detections[:, 3] - detections[:, 1])\n",
        "        return detections[areas >= self.min_box_area]\n",
        "    \n",
        "    def _init_track(self, detection):\n",
        "        \"\"\"Initialize a new track from a detection.\"\"\"\n",
        "        self.track_id_count += 1\n",
        "        return {\n",
        "            'track_id': self.track_id_count,\n",
        "            'bbox': detection[:4].tolist(),\n",
        "            'score': float(detection[4]),\n",
        "            'class_id': int(detection[5]),\n",
        "            'frame_id': self.frame_id,\n",
        "            'state': 'new'\n",
        "        }\n",
        "    \n",
        "    def _associate(self, tracks, detections):\n",
        "        \"\"\"\n",
        "        Associate detections with tracks using IoU matching.\n",
        "        Returns: (matched_pairs, unmatched_track_indices, unmatched_det_indices)\n",
        "        \"\"\"\n",
        "        if len(tracks) == 0 or len(detections) == 0:\n",
        "            return [], list(range(len(tracks))), list(range(len(detections)))\n",
        "        \n",
        "        # Convert tracks to array for IoU computation\n",
        "        track_boxes = np.array([t['bbox'] for t in tracks.values()])\n",
        "        det_boxes = detections[:, :4]\n",
        "        \n",
        "        # Compute IoU matrix\n",
        "        iou_matrix = self._compute_iou_matrix(track_boxes, det_boxes)\n",
        "        \n",
        "        # Greedy matching\n",
        "        matched = []\n",
        "        unmatched_tracks = list(range(len(tracks)))\n",
        "        unmatched_dets = list(range(len(detections)))\n",
        "        \n",
        "        while unmatched_tracks and unmatched_dets:\n",
        "            max_iou = -1\n",
        "            best_track = -1\n",
        "            best_det = -1\n",
        "            \n",
        "            for t in unmatched_tracks:\n",
        "                for d in unmatched_dets:\n",
        "                    if iou_matrix[t, d] > max_iou:\n",
        "                        max_iou = iou_matrix[t, d]\n",
        "                        best_track = t\n",
        "                        best_det = d\n",
        "            \n",
        "            if max_iou >= self.match_thresh:\n",
        "                matched.append((best_track, best_det))\n",
        "                unmatched_tracks.remove(best_track)\n",
        "                unmatched_dets.remove(best_det)\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        return matched, unmatched_tracks, unmatched_dets\n",
        "    \n",
        "    @staticmethod\n",
        "    def _compute_iou_matrix(boxes1, boxes2):\n",
        "        \"\"\"Compute IoU matrix between two sets of boxes.\"\"\"\n",
        "        area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
        "        area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
        "        \n",
        "        x1 = np.maximum(boxes1[:, 0:1], boxes2[:, 0])\n",
        "        y1 = np.maximum(boxes1[:, 1:2], boxes2[:, 1])\n",
        "        x2 = np.minimum(boxes1[:, 2:3], boxes2[:, 2])\n",
        "        y2 = np.minimum(boxes1[:, 3:4], boxes2[:, 3])\n",
        "        \n",
        "        intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n",
        "        union = area1[:, np.newaxis] + area2 - intersection\n",
        "        \n",
        "        return intersection / (union + 1e-6)\n",
        "    \n",
        "    def get_track_count(self):\n",
        "        \"\"\"Get total number of tracks created.\"\"\"\n",
        "        return self.track_id_count\n",
        "\n",
        "class TrackManager:\n",
        "    \"\"\"Manages tracks and determines when to generate 3D cuboids.\"\"\"\n",
        "    def __init__(self, min_track_length=3):\n",
        "        self.min_track_length = min_track_length\n",
        "        self.track_history = defaultdict(list)\n",
        "        self.cuboid_generated = set()\n",
        "    \n",
        "    def update(self, tracked_objects):\n",
        "        tracks_for_cuboid = []\n",
        "        for track in tracked_objects:\n",
        "            track_id = track['track_id']\n",
        "            self.track_history[track_id].append({\n",
        "                'bbox': track['bbox'],\n",
        "                'score': track['score'],\n",
        "                'class_id': track['class_id'],\n",
        "                'state': track['state']\n",
        "            })\n",
        "            track_length = len(self.track_history[track_id])\n",
        "            if track_id not in self.cuboid_generated and track_length >= self.min_track_length:\n",
        "                tracks_for_cuboid.append({**track, 'reason': 'new_stable'})\n",
        "                self.cuboid_generated.add(track_id)\n",
        "            elif track['state'] == 're-identified' and track_id in self.cuboid_generated:\n",
        "                tracks_for_cuboid.append({**track, 'reason': 'updated'})\n",
        "        return tracks_for_cuboid\n",
        "\n",
        "def yolo_to_bytetrack_format(boxes, scores, classes):\n",
        "    if len(boxes) == 0:\n",
        "        return np.empty((0, 6))\n",
        "    return np.column_stack([boxes, scores, classes])\n",
        "\n",
        "print('✅ ByteTrack tracker classes loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n0YRNykFGQ2i",
      "metadata": {
        "id": "n0YRNykFGQ2i"
      },
      "source": [
        "<h2>2. Extracting 2D Bounding Box Coordinates using YOLOv8x weight (Exp 2)</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d76ec13d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ByteTrack initialized!\n"
          ]
        }
      ],
      "source": [
        "# Initialize ByteTrack tracker\n",
        "tracker = SimpleByteTracker(track_thresh=0.5, track_buffer=30, match_thresh=0.35)\n",
        "track_manager = TrackManager(min_track_length=3)\n",
        "tracking_stats = {'total_detections': 0, 'total_tracks': 0}\n",
        "cuboid_cache = {}  # Cache cuboids by track_id\n",
        "print('✅ ByteTrack initialized!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "70ce09e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract2D(bytetrack=False, model_path=None, input_dir=None, output_dir=None, \n",
        "              conf_thresh=0.6, iou_thresh=0.45, \n",
        "              track_thresh=0.5, track_buffer=30, match_thresh=0.35):\n",
        "    \n",
        "    # Use global paths if not provided\n",
        "    _model_path = Path(model_path) if model_path else Path(model_weights)\n",
        "    _input_dir = Path(input_dir) if input_dir else seq_dir\n",
        "    _output_dir = Path(output_dir) if output_dir else bbox_output_dir\n",
        "    \n",
        "    # Create output directory\n",
        "    _output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Initialize tracker if ByteTrack is enabled\n",
        "    tracker = None\n",
        "    track_manager = None\n",
        "    tracking_stats = {'total_detections': 0, 'total_tracks': 0}\n",
        "    \n",
        "    if bytetrack:\n",
        "        tracker = SimpleByteTracker(\n",
        "            track_thresh=track_thresh,\n",
        "            track_buffer=track_buffer,\n",
        "            match_thresh=match_thresh\n",
        "        )\n",
        "        track_manager = TrackManager(min_track_length=3)\n",
        "        print('✅ ByteTrack initialized!')\n",
        "    else:\n",
        "        print('✅ Running without ByteTrack')\n",
        "    \n",
        "    # Load YOLO model\n",
        "    model = YOLO(str(_model_path))\n",
        "    print(f'✅ Model loaded: {_model_path}')\n",
        "    \n",
        "    # Get all image files\n",
        "    image_files = sorted(_input_dir.glob('*.png'))\n",
        "    print(f'Processing {len(image_files)} images...')\n",
        "    \n",
        "    # Track detection ID counter for non-ByteTrack mode\n",
        "    detection_id_counter = 0\n",
        "    \n",
        "    # Inference on images\n",
        "    for frame_idx, img_file in enumerate(tqdm(image_files, desc='Detecting & Tracking' if bytetrack else 'Detecting')):\n",
        "        results = model(str(img_file), conf=conf_thresh, iou=iou_thresh, verbose=False)\n",
        "        \n",
        "        # Extract detections\n",
        "        boxes_list = []\n",
        "        scores_list = []\n",
        "        classes_list = []\n",
        "        \n",
        "        for result in results:\n",
        "            boxes = result.boxes.xyxy.cpu().numpy()  # [x_min, y_min, x_max, y_max]\n",
        "            classes = result.boxes.cls.cpu().numpy()\n",
        "            confs = result.boxes.conf.cpu().numpy()\n",
        "            \n",
        "            for i in range(len(boxes)):\n",
        "                boxes_list.append(boxes[i])\n",
        "                scores_list.append(confs[i])\n",
        "                classes_list.append(classes[i])\n",
        "        \n",
        "        # Prepare frame data\n",
        "        frame_data = []\n",
        "        \n",
        "        if bytetrack:\n",
        "            # ByteTrack mode: assign track IDs\n",
        "            if len(boxes_list) > 0:\n",
        "                detections = yolo_to_bytetrack_format(\n",
        "                    np.array(boxes_list),\n",
        "                    np.array(scores_list),\n",
        "                    np.array(classes_list)\n",
        "                )\n",
        "            else:\n",
        "                detections = np.empty((0, 6))\n",
        "            \n",
        "            tracked_objects = tracker.update(detections)\n",
        "            tracking_stats['total_detections'] += len(detections)\n",
        "            tracking_stats['total_tracks'] = tracker.get_track_count()\n",
        "            \n",
        "            # Format output with track_id\n",
        "            for track in tracked_objects:\n",
        "                # Handle bbox: convert to list if it's a numpy array\n",
        "                bbox = track['bbox']\n",
        "                if isinstance(bbox, np.ndarray):\n",
        "                    bbox = bbox.tolist()\n",
        "                elif not isinstance(bbox, list):\n",
        "                    bbox = list(bbox)  # Handle other array-like types\n",
        "                \n",
        "                obj_output = {\n",
        "                    'track_id': track['track_id'],\n",
        "                    'class': track['class_id'],\n",
        "                    'confidence': track['score'],\n",
        "                    'bbox': bbox\n",
        "                }\n",
        "                frame_data.append(obj_output)\n",
        "        else:\n",
        "            # Non-ByteTrack mode: no track IDs \n",
        "            for i in range(len(boxes_list)):\n",
        "                obj_output = {\n",
        "                    'class': int(classes_list[i]),\n",
        "                    'confidence': float(scores_list[i]),\n",
        "                    'bbox': boxes_list[i].tolist()\n",
        "                }\n",
        "                frame_data.append(obj_output)\n",
        "                detection_id_counter += 1\n",
        "        \n",
        "        # Save to JSON\n",
        "        json_path = _output_dir / f'{img_file.stem}.json'\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(frame_data, f, indent=4)\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f'\\n✅ 2D bounding box extraction complete!')\n",
        "    print(f'   Total frames processed: {len(image_files)}')\n",
        "    print(f'   Output directory: {_output_dir}')\n",
        "    \n",
        "    if bytetrack:\n",
        "        print(f'   Total detections: {tracking_stats[\"total_detections\"]}')\n",
        "        print(f'   Total unique tracks: {tracking_stats[\"total_tracks\"]}')\n",
        "        return tracking_stats\n",
        "    else:\n",
        "        print(f'   Total detections: {detection_id_counter}')\n",
        "        return {'total_detections': detection_id_counter}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e180fb81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ByteTrack initialized!\n",
            "✅ Model loaded: ../results/yolov8s/kitti/best.pt\n",
            "Processing 4071 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detecting & Tracking: 100%|██████████| 4071/4071 [05:30<00:00, 12.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ 2D bounding box extraction complete!\n",
            "   Total frames processed: 4071\n",
            "   Output directory: ../data/bbox_outputs_bytetrack\n",
            "   Total detections: 12214\n",
            "   Total unique tracks: 2298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'total_detections': 12214, 'total_tracks': 2298}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract2D(bytetrack=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pxlYIJNTHRhk",
      "metadata": {
        "id": "pxlYIJNTHRhk"
      },
      "source": [
        "<h2>3. Converting 2D Bounding Boxes to 3D Cuboids</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c9b7e0bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 4071 camera poses for SLAM integration.\n"
          ]
        }
      ],
      "source": [
        "# 1. (Optional) Load camera poses for each frame (SLAM trajectory or KITTI ground truth)\n",
        "\n",
        "poses = {}\n",
        "if pose_dir.exists():\n",
        "    # Each line in pose_file is a 3x4 transform matrix (T_cam_world) flattened\n",
        "    with open(pose_dir, 'r') as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            vals = list(map(float, line.split()))\n",
        "            if len(vals) == 12:  # valid pose line\n",
        "                Tcw = np.array(vals).reshape(3, 4)     # Transform from world to camera\n",
        "                Rcw = Tcw[:, :3]                       # rotation matrix (camera <- world)\n",
        "                tcw = Tcw[:, 3]                        # translation vector (camera origin in world coords, in camera frame)\n",
        "                # Compute world-to-camera inverse: camera-to-world (Rwc, twc)\n",
        "                Rwc = Rcw.T\n",
        "                twc = -Rwc.dot(tcw)\n",
        "                poses[idx] = (Rwc, twc)\n",
        "    print(f'Loaded {len(poses)} camera poses for SLAM integration.')\n",
        "else:\n",
        "    print('Camera pose file not found. Results will be in camera coordinates.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "04cfdbd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Function to estimate 3D cuboid from a single 2D bounding box\n",
        "\n",
        "def estimate_cuboid_from_bbox(bbox, class_id):\n",
        "    \"\"\"\n",
        "    Given a 2D bounding box [x_min, y_min, x_max, y_max] and object class,\n",
        "    estimate the 3D cuboid's center (camera coords), orientation (yaw), and dimensions.\n",
        "    Returns (X, Y, Z, yaw, width, height, length).\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    W, H, L = class_dims.get(class_id, {'dims': (1.0, 1.0, 1.0)})['dims']  # default to 1m cube if class unknown\n",
        "    # **Depth estimation**: use bounding box height to approximate distance\n",
        "    pixel_height = y2 - y1\n",
        "    Z = (fy * H) / (pixel_height + 1e-6)   # depth along camera Z-axis (in meters)\n",
        "    # **Horizontal position**: assume bounding box center corresponds to object center horizontally\n",
        "    u_center = 0.5 * (x1 + x2)\n",
        "    X = (u_center - cx) / fx * Z           # lateral position in camera coords (X axis)\n",
        "    # **Vertical position**: compute Y so that bottom of cuboid aligns with detected bottom\n",
        "    v_bottom = max(y1, y2)\n",
        "    Y = (v_bottom - cy) / fy * Z - 0.5 * H  # vertical position (camera Y-axis, positive downwards)\n",
        "    # **Orientation (yaw) estimation**: solve for yaw such that projected width fits the 2D box width\n",
        "    pixel_width = x2 - x1\n",
        "    # Use geometry: for a given yaw, the effective horizontal span ≈ |cos(yaw)*W + sin(yaw)*L| in world units:contentReference[oaicite:10]{index=10}.\n",
        "    # We find yaw that matches the observed width ~ (pixel_width/fx)*Z.\n",
        "    target_hspan = (pixel_width / fx) * Z  # horizontal span in meters that the box suggests\n",
        "    # Solve for yaw using the equation: |cos(yaw)*W + sin(yaw)*L| = target_hspan\n",
        "    # (We consider yaw in [0, pi/2] since symmetric; will decide left/right later.)\n",
        "    # Avoiding negative sqrt issues:\n",
        "    cos_yaw = 0.0\n",
        "    if target_hspan < L:\n",
        "        # Quadratic solve: cos_yaw * W + sin_yaw * L = target_hspan\n",
        "        # => (L^2+W^2)*sin^2(yaw) - 2*W*target_hspan*sin(yaw) + (W^2 - target_hspan^2) = 0 in terms of sin(yaw).\n",
        "        # We solve for cos(yaw) instead via: cos_yaw = sqrt(1 - sin^2(yaw)).\n",
        "        # We'll do a simple bracket search since analytic might be complex with abs.\n",
        "        pass  # (we'll handle via iteration below)\n",
        "    # Instead of closed-form, do a small search over yaw\n",
        "    best_yaw = 0.0\n",
        "    min_err = float('inf')\n",
        "    for deg in range(0, 91, 5):  # coarse search every 5 degrees\n",
        "        yaw_cand = np.deg2rad(deg)\n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw_cand)\n",
        "        proj_width = u_max - u_min\n",
        "        err = abs(proj_width - pixel_width)\n",
        "        if err < min_err:\n",
        "            min_err = err\n",
        "            best_yaw = yaw_cand\n",
        "    # Refine yaw around best_yaw\n",
        "    yaw = best_yaw\n",
        "    for _ in range(3):  # a few refinement iterations\n",
        "        delta = np.deg2rad(2)  # small adjustment step (~2 degrees)\n",
        "        # Try adjusting yaw slightly up or down to see if error improves\n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw)\n",
        "        err0 = abs((u_max - u_min) - pixel_width)\n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw + delta); err_plus = abs((u_max - u_min) - pixel_width)\n",
        "        u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw - delta); err_minus = abs((u_max - u_min) - pixel_width)\n",
        "        # Gradient descent: move yaw in direction of decreasing error\n",
        "        if err_plus < err0 or err_minus < err0:\n",
        "            if err_plus < err_minus:\n",
        "                yaw += delta\n",
        "            else:\n",
        "                yaw -= delta\n",
        "        else:\n",
        "            break  # no improvement\n",
        "    # **Center adjustment**: re-align X if needed so projected box is centered on detection\n",
        "    u_min, u_max, _, _ = project_cuboid(X, Y, Z, W, H, L, yaw)\n",
        "    proj_center = 0.5 * (u_min + u_max)\n",
        "    X += ((u_center - proj_center) / fx) * Z  # small tweak to center alignment\n",
        "\n",
        "    return X, Y, Z, yaw, W, H, L"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ecb53fd",
      "metadata": {},
      "source": [
        "<b>CubeSLAM technique</b>\n",
        "\n",
        "1. Single-view 3D cuboid proposal:\n",
        "- What CubeSLAM does: Given a 2D bounding box + class prior (average dimensions per category), estimate a plausible 3D cuboid (center, yaw, dimensions) in camera frame.\n",
        "- Corresponding code snippet: `estimate_cuboid_from_bbox()` and `project_cuboid()`.\n",
        "\n",
        "2. Multi-view optimization/SLAM fusion:\n",
        "- What CubeSLAM does: Refines cuboids jointly with camera poses using bundle adjustment constraints.\n",
        "- Corresponding code: This part is in ORB-SLAM3, but ORB-SLAM3 doesn’t yet include cuboid objects by default. Need to add manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "673d6ed1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total frames in sequence: 4071\n"
          ]
        }
      ],
      "source": [
        "num_frames = len(list(seq_dir.glob(\"*.png\")))\n",
        "# frame_times = np.arange(num_frames) / fps\n",
        "frame_times = np.loadtxt(str(seq_dir.parent / 'times.txt'))\n",
        "print(f\"Total frames in sequence: {num_frames}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7eb738de",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate3D_cuboids(bbox_dir=None, output_dir=None, use_tracking=False, num_frames=None):\n",
        "    # Use global paths if not provided\n",
        "    _bbox_dir = Path(bbox_dir) if bbox_dir else bbox_output_dir\n",
        "    _output_dir = Path(output_dir) if output_dir else cuboid_output_dir\n",
        "    \n",
        "    # Create output directory\n",
        "    _output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Get all bbox files\n",
        "    bbox_files = sorted(_bbox_dir.glob('*.json'), key=lambda x: int(x.stem))\n",
        "    \n",
        "    # Determine frames to process\n",
        "    if num_frames is None:\n",
        "        frames_to_process = range(len(bbox_files))\n",
        "        print(f'Processing all {len(bbox_files)} frames...')\n",
        "    else:\n",
        "        frames_to_process = range(min(num_frames, len(bbox_files)))\n",
        "        print(f'Processing {len(frames_to_process)} frames...')\n",
        "    \n",
        "    # Initialize dimension cache (only if tracking is enabled)\n",
        "    dimension_cache = {}\n",
        "    cache_stats = {'cache_hits': 0, 'cache_misses': 0, 'no_cache': 0}\n",
        "    \n",
        "    if use_tracking:\n",
        "        print('✅ Dimension caching ENABLED (tracking mode)')\n",
        "    else:\n",
        "        print('✅ Dimension caching DISABLED (no tracking mode)')\n",
        "    \n",
        "    # Process each frame\n",
        "    for frame_id in tqdm(frames_to_process, desc='Generating 3D Cuboids'):\n",
        "        # Find corresponding bbox file\n",
        "        bbox_file = None\n",
        "        for f in bbox_files:\n",
        "            if int(f.stem) == frame_id:\n",
        "                bbox_file = f\n",
        "                break\n",
        "        \n",
        "        if bbox_file:\n",
        "            with open(bbox_file, 'r') as f:\n",
        "                detections = json.load(f)\n",
        "            \n",
        "            output_data = {'frame': frame_id, 'objects': []}\n",
        "            \n",
        "            for det in detections:\n",
        "                # Get track_id or detection_id\n",
        "                track_id = det.get('track_id')  # Will be None if tracking wasn't used\n",
        "                detection_id = det.get('detection_id')  # Will be None if tracking was used\n",
        "                \n",
        "                cls_id = det['class']\n",
        "                conf = det.get('confidence', 1.0)\n",
        "                bbox = det['bbox']\n",
        "                \n",
        "                # ALWAYS generate position and rotation from current bbox\n",
        "                Xc, Yc, Zc, yaw, W_new, H_new, L_new = estimate_cuboid_from_bbox(bbox, cls_id)\n",
        "                \n",
        "                # Use CAMERA coordinates\n",
        "                x_cam, y_cam, z_cam = float(Xc), float(Yc), float(Zc)\n",
        "                \n",
        "                # Rotation quaternion from current yaw\n",
        "                half_yaw = yaw / 2.0\n",
        "                rot = {\n",
        "                    'w': float(np.cos(half_yaw)),\n",
        "                    'x': 0.0,\n",
        "                    'y': float(np.sin(half_yaw)),\n",
        "                    'z': 0.0\n",
        "                }\n",
        "                \n",
        "                # Handle dimensions based on tracking mode\n",
        "                if use_tracking and track_id is not None:\n",
        "                    # Tracking mode: Use dimension caching\n",
        "                    if track_id in dimension_cache:\n",
        "                        # Reuse cached dimensions (more stable)\n",
        "                        W, H, L = dimension_cache[track_id]\n",
        "                        cache_stats['cache_hits'] += 1\n",
        "                    else:\n",
        "                        # Use newly estimated dimensions and cache them\n",
        "                        W, H, L = float(W_new), float(H_new), float(L_new)\n",
        "                        dimension_cache[track_id] = (W, H, L)\n",
        "                        cache_stats['cache_misses'] += 1\n",
        "                else:\n",
        "                    # No tracking mode: Always use fresh dimensions\n",
        "                    W, H, L = float(W_new), float(H_new), float(L_new)\n",
        "                    cache_stats['no_cache'] += 1\n",
        "                \n",
        "                # Prepare object record\n",
        "                obj_record = {\n",
        "                    'class': class_dims.get(cls_id, {'name': str(cls_id)})['name'],\n",
        "                    'class_id': int(cls_id),\n",
        "                    'confidence': float(conf),\n",
        "                    'center': [x_cam, y_cam, z_cam],      # Updated every frame\n",
        "                    'rotation': rot,                       # Updated every frame\n",
        "                    'dimensions': [W, H, L]                # Stable if cached, fresh otherwise\n",
        "                }\n",
        "                \n",
        "                # Add ID field based on mode\n",
        "                if use_tracking and track_id is not None:\n",
        "                    obj_record['track_id'] = track_id\n",
        "                elif detection_id is not None:\n",
        "                    obj_record['detection_id'] = detection_id\n",
        "                \n",
        "                output_data['objects'].append(obj_record)\n",
        "            \n",
        "            # Save to JSON\n",
        "            out_path = _output_dir / f'{frame_id:06d}.json'\n",
        "            with open(out_path, 'w') as f:\n",
        "                json.dump(output_data, f, indent=4)\n",
        "        else:\n",
        "            print(f'Warning: No bbox data for frame {frame_id}')\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f'\\n✅ 3D Cuboid generation complete!')\n",
        "    print(f'   Total frames processed: {len(frames_to_process)}')\n",
        "    print(f'   Output directory: {_output_dir}')\n",
        "    \n",
        "    if use_tracking:\n",
        "        print(f'   Total unique tracks with cached dimensions: {len(dimension_cache)}')\n",
        "        print(f'   Cache hits: {cache_stats[\"cache_hits\"]}')\n",
        "        print(f'   Cache misses: {cache_stats[\"cache_misses\"]}')\n",
        "        if cache_stats['cache_hits'] + cache_stats['cache_misses'] > 0:\n",
        "            hit_rate = cache_stats['cache_hits'] / (cache_stats['cache_hits'] + cache_stats['cache_misses']) * 100\n",
        "            print(f'   Dimension cache hit rate: {hit_rate:.1f}%')\n",
        "    else:\n",
        "        print(f'   Total detections processed: {cache_stats[\"no_cache\"]}')\n",
        "        print(f'   Dimension caching: DISABLED')\n",
        "    \n",
        "    return cache_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6e900332",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing all 4071 frames...\n",
            "✅ Dimension caching ENABLED (tracking mode)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating 3D Cuboids: 100%|██████████| 4071/4071 [00:07<00:00, 524.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ 3D Cuboid generation complete!\n",
            "   Total frames processed: 4071\n",
            "   Output directory: ../data/cuboid_outputs_bytetrack\n",
            "   Total unique tracks with cached dimensions: 2298\n",
            "   Cache hits: 9916\n",
            "   Cache misses: 2298\n",
            "   Dimension cache hit rate: 81.2%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'cache_hits': 9916, 'cache_misses': 2298, 'no_cache': 0}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate3D_cuboids(bbox_dir=bbox_output_dir, output_dir=cuboid_output_dir, use_tracking=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0fec7b09",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: 2189 poses across 4071 frames = 53.8% coverage\n",
            "After: 1806 poses across 4071 frames = 44.4% coverage\n"
          ]
        }
      ],
      "source": [
        "# Compare frame coverage\n",
        "print(f\"Before: {2189} poses across {4071} frames = {2189/4071*100:.1f}% coverage\")\n",
        "print(f\"After: {1806} poses across {4071} frames = {1806/4071*100:.1f}% coverage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "770bca7d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total keyframes: 1298\n",
            "Large jumps (>20m): 0\n",
            "Jump frames: None\n",
            "Mean inter-frame distance: 0.37m\n",
            "Std inter-frame distance: 0.27m\n",
            "→ Very small movements suggest scale drift!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "kf = np.loadtxt('../third_party/ORB_SLAM3/Examples/Monocular/KeyFrameTrajectory.txt')\n",
        "trans = kf[:, [3, 7, 11]]\n",
        "\n",
        "# Check for jumps (tracking loss indicators)\n",
        "diffs = np.linalg.norm(np.diff(trans, axis=0), axis=1)\n",
        "large_jumps = np.where(diffs > 20)[0]  # >20m jumps suspicious\n",
        "\n",
        "print(f\"Total keyframes: {len(kf)}\")\n",
        "print(f\"Large jumps (>20m): {len(large_jumps)}\")\n",
        "print(f\"Jump frames: {large_jumps[:10] if len(large_jumps) > 0 else 'None'}\")\n",
        "\n",
        "# Check trajectory smoothness\n",
        "print(f\"Mean inter-frame distance: {diffs.mean():.2f}m\")\n",
        "print(f\"Std inter-frame distance: {diffs.std():.2f}m\")\n",
        "\n",
        "if len(large_jumps) > 10:\n",
        "    print(\"→ Many jumps suggest tracking loss!\")\n",
        "if diffs.mean() < 1.0:\n",
        "    print(\"→ Very small movements suggest scale drift!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
